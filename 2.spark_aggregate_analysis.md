# L'opération aggregate dans Spark

## Définition

`aggregate` est une **action** de réduction très puissante et flexible qui permet d'agréger les éléments d'un RDD en une valeur d'un type potentiellement différent, en utilisant deux fonctions distinctes.

## Signature

```scala
def aggregate[U](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U
```

### Paramètres
- **`zeroValue: U`** : Valeur initiale neutre
- **`seqOp: (U, T) => U`** : Fonction séquentielle (combine accumulateur + élément)
- **`combOp: (U, U) => U`** : Fonction de combinaison (combine deux accumulateurs)

## Comment ça fonctionne

### Étapes d'exécution
1. **Phase locale** : Sur chaque partition, utilise `seqOp` avec `zeroValue`
2. **Phase de combinaison** : Combine les résultats des partitions avec `combOp`

```scala
// Exemple simple : calculer la somme
val numbers = sc.parallelize(List(1, 2, 3, 4, 5, 6), 3)

val sum = numbers.aggregate(0)(
  seqOp = (acc, value) => acc + value,    // Additionne dans chaque partition
  combOp = (acc1, acc2) => acc1 + acc2    // Combine les résultats des partitions
)
// Résultat: 21
```

## Exécution distribuée détaillée

```
RDD: [1, 2, 3, 4, 5, 6] avec 3 partitions

Partition 1: [1, 2]
zeroValue=0 → seqOp(0,1)=1 → seqOp(1,2)=3

Partition 2: [3, 4]  
zeroValue=0 → seqOp(0,3)=3 → seqOp(3,4)=7

Partition 3: [5, 6]
zeroValue=0 → seqOp(0,5)=5 → seqOp(5,6)=11

Combinaison finale:
zeroValue=0 → combOp(0,3)=3 → combOp(3,7)=10 → combOp(10,11)=21
```

## Exemples pratiques

### Exemple 1 : Statistiques complètes
```scala
case class Stats(sum: Long, count: Long, min: Int, max: Int)

val numbers = sc.parallelize(List(10, 5, 20, 15, 30, 8))

val stats = numbers.aggregate(Stats(0, 0, Int.MaxValue, Int.MinValue))(
  seqOp = (acc, value) => Stats(
    sum = acc.sum + value,
    count = acc.count + 1,
    min = math.min(acc.min, value),
    max = math.max(acc.max, value)
  ),
  combOp = (acc1, acc2) => Stats(
    sum = acc1.sum + acc2.sum,
    count = acc1.count + acc2.count,
    min = math.min(acc1.min, acc2.min),
    max = math.max(acc1.max, acc2.max)
  )
)

println(stats)  // Stats(88, 6, 5, 30)
val average = stats.sum.toDouble / stats.count  // 14.67
```

### Exemple 2 : Collecte de mots uniques
```scala
val words = sc.parallelize(List("spark", "scala", "spark", "hadoop", "scala", "python"))

val uniqueWords = words.aggregate(Set.empty[String])(
  seqOp = (set, word) => set + word,      // Ajoute le mot au Set
  combOp = (set1, set2) => set1 ++ set2   // Union des Sets
)

println(uniqueWords)  // Set(spark, scala, hadoop, python)
```

### Exemple 3 : Comptage par longueur de mots
```scala
val sentences = sc.parallelize(List("Hello world", "Spark is great", "Scala programming"))

val wordLengthCounts = sentences.aggregate(Map.empty[Int, Int])(
  seqOp = (map, sentence) => {
    sentence.split(" ").foldLeft(map) { (acc, word) =>
      val length = word.length
      acc + (length -> (acc.getOrElse(length, 0) + 1))
    }
  },
  combOp = (map1, map2) => {
    (map1.keySet ++ map2.keySet).map { key =>
      key -> (map1.getOrElse(key, 0) + map2.getOrElse(key, 0))
    }.toMap
  }
)

println(wordLengthCounts)  // Map(5 -> 3, 2 -> 1, 7 -> 1, 11 -> 1)
```

### Exemple 4 : Moyenne (type différent)
```scala
val numbers = sc.parallelize(List(10, 20, 30, 40, 50))

// Accumulateur: (somme, count)
val (totalSum, totalCount) = numbers.aggregate((0.0, 0))(
  seqOp = (acc, value) => (acc._1 + value, acc._2 + 1),
  combOp = (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
)

val average = totalSum / totalCount
println(s"Moyenne: $average")  // Moyenne: 30.0
```

## Différences avec d'autres opérations

| Opération | Type de sortie | Flexibilité | Cas d'usage |
|-----------|----------------|-------------|-------------|
| **`reduce`** | Même type que RDD | Limitée | Agrégations simples du même type |
| **`fold`** | Même type que RDD | Moyenne | Comme reduce avec valeur initiale |
| **`aggregate`** | Type différent possible | Maximale | Agrégations complexes, statistiques |

### Comparaison avec reduce
```scala
val numbers = sc.parallelize(List(1, 2, 3, 4, 5))

// reduce - même type en entrée/sortie
val sum1 = numbers.reduce(_ + _)  // Int → Int

// aggregate - peut changer de type
val sum2 = numbers.aggregate(0.0)(  // Int → Double
  (acc, value) => acc + value,
  (acc1, acc2) => acc1 + acc2
)
```

## Contraintes importantes

### 1. Fonctions doivent être associatives et commutatives
```scala
// ✅ Correct - addition est associative et commutative
numbers.aggregate(0)(_ + _, _ + _)

// ❌ Problématique - soustraction n'est pas commutative
numbers.aggregate(0)(_ - _, _ - _)  // Résultat imprévisible !
```

### 2. ZeroValue utilisé multiple fois
```scala
val numbers = sc.parallelize(List(1, 2, 3, 4), 2)

// ❌ Attention : zeroValue=10 utilisé 3 fois (2 partitions + 1 combinaison)
val result = numbers.aggregate(10)(_ + _, _ + _)
// Partition 1: 10 + 1 + 2 = 13
// Partition 2: 10 + 3 + 4 = 17  
// Final: 10 + 13 + 17 = 40 (pas 20 !)
```

## Avantages d'aggregate

### ✅ Avantages
- **Flexibilité de type** : Peut retourner un type différent du RDD
- **Performance** : Parallélisable et distribué
- **Puissance** : Peut implémenter des agrégations complexes
- **Mémoire efficace** : Pas besoin de collecter toutes les données

### ⚠️ Précautions
- Fonctions doivent être associatives et commutatives
- ZeroValue doit être neutre
- Plus complexe que `reduce` ou `fold`

## Cas d'usage typiques

1. **Calcul de statistiques** (moyenne, variance, min, max)
2. **Histogrammes et comptages**
3. **Collecte d'éléments uniques**
4. **Agrégations par groupes**
5. **Transformations de type** (RDD[String] → Map[Char, Int])

## Bonnes pratiques

### ✅ DO
```scala
// Utiliser des valeurs neutres
.aggregate(0)  // pour addition
.aggregate(1)  // pour multiplication
.aggregate(Set.empty)  // pour union
.aggregate(Map.empty)  // pour combinaison de maps
```

### ❌ DON'T
```scala
// Éviter les valeurs non-neutres
.aggregate(10)  // va être ajouté plusieurs fois !

// Éviter les opérations non-commutatives
.aggregate("")((acc, s) => acc + s, (s1, s2) => s1 + s2)  // ordre imprévisible
```

## Résumé

`aggregate` est l'opération de réduction la plus puissante dans Spark, permettant de créer des agrégations complexes avec changement de type, tout en bénéficiant de la distribution et du parallélisme. C'est l'outil idéal pour des calculs statistiques avancés sur de gros volumes de données.