# Transformations RDD

Le tableau suivant liste certaines des transformations communes supportées par Spark. Référez-vous à la documentation de l'API RDD ([Scala](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html), [Java](https://spark.apache.org/docs/latest/api/java/), [Python](https://spark.apache.org/docs/latest/api/python/), [R](https://spark.apache.org/docs/latest/api/R/)) et à la documentation de l'API PairRDD ([Scala](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html), [Java](https://spark.apache.org/docs/latest/api/java/)) pour plus de détails.

| Transformation | Signification |
|----------------|---------------|
| **map**(_func_) | Retourne un nouveau dataset distribué formé en passant chaque élément de la source à travers une fonction _func_. |
| **filter**(_func_) | Retourne un nouveau dataset formé en sélectionnant les éléments de la source sur lesquels _func_ retourne true. |
| **flatMap**(_func_) | Similaire à map, mais chaque élément d'entrée peut être mappé vers 0 ou plusieurs éléments de sortie (donc _func_ devrait retourner une séquence plutôt qu'un seul élément). |
| **mapPartitions**(_func_) | Similaire à map, mais s'exécute séparément sur chaque partition (bloc) du RDD, donc _func_ doit être de type Iterator<T> => Iterator<U> lors de l'exécution sur un RDD de type T. |
| **mapPartitionsWithIndex**(_func_) | Similaire à mapPartitions, mais fournit également à _func_ un entier représentant l'index de la partition, donc _func_ doit être de type (Int, Iterator<T>) => Iterator<U> lors de l'exécution sur un RDD de type T. |
| **sample**(_withReplacement_, _fraction_, _seed_) | Échantillonne une fraction _fraction_ des données, avec ou sans remplacement, en utilisant une graine de nombre aléatoire donnée. |
| **union**(_otherDataset_) | Retourne un nouveau dataset qui contient l'union des éléments dans le dataset source et l'argument. |
| **intersection**(_otherDataset_) | Retourne un nouveau RDD qui contient l'intersection des éléments dans le dataset source et l'argument. |
| **distinct**([_numPartitions_]) | Retourne un nouveau dataset qui contient les éléments distincts du dataset source. |
| **groupByKey**([_numPartitions_]) | Quand appelé sur un dataset de paires (K,V), retourne un dataset de paires (K, Iterable<V>).<br/>**Note :** Si vous groupez afin d'effectuer une agrégation (comme une somme ou une moyenne) sur chaque clé, utiliser `reduceByKey` ou `aggregateByKey` donnera de meilleures performances.<br/>**Note :** Par défaut, le niveau de parallélisme dans la sortie dépend du nombre de partitions du RDD parent. Vous pouvez passer un argument optionnel `numPartitions` pour définir un nombre différent de tâches. |
| **reduceByKey**(_func_, [_numPartitions_]) | Quand appelé sur un dataset de paires (K,V), retourne un dataset de paires (K,V) où les valeurs pour chaque clé sont agrégées en utilisant la fonction de réduction donnée _func_, qui doit être de type (V,V) => V. Comme dans groupByKey, le nombre de tâches de réduction est configurable via un second argument optionnel. |
| **aggregateByKey**(_zeroValue_)(_seqOp_, _combOp_, [_numPartitions_]) | Quand appelé sur un dataset de paires (K,V), retourne un dataset de paires (K,U) où les valeurs pour chaque clé sont agrégées en utilisant les fonctions de combinaison données et une valeur zéro neutre. Permet un type de valeur agrégée différent du type de valeur d'entrée, tout en évitant les allocations inutiles. Comme dans groupByKey, le nombre de tâches de réduction est configurable via un argument optionnel. |
| **sortByKey**([_ascending_], [_numPartitions_]) | Quand appelé sur un dataset de paires (K,V) où K implémente Ordered, retourne un dataset de paires (K,V) triées par clés en ordre croissant ou décroissant, comme spécifié dans l'argument booléen `ascending`. |
| **join**(_otherDataset_, [_numPartitions_]) | Quand appelé sur des datasets de type (K,V) et (K,W), retourne un dataset de paires (K,(V,W)) avec toutes les paires d'éléments pour chaque clé. Les jointures externes sont supportées via `leftOuterJoin`, `rightOuterJoin`, et `fullOuterJoin`. |
| **cogroup**(_otherDataset_, [_numPartitions_]) | Quand appelé sur des datasets de type (K,V) et (K,W), retourne un dataset de tuples (K,(Iterable<V>, Iterable<W>)). Cette opération est aussi appelée `groupWith`. |
| **cartesian**(_otherDataset_) | Quand appelé sur des datasets de types T et U, retourne un dataset de paires (T,U) de toutes les paires d'éléments. |
| **pipe**(_command_, _[envVars]_) | Fait passer chaque partition du RDD à travers une commande shell, par exemple un script Perl ou bash. Les éléments RDD sont écrits sur stdin du processus et les lignes sorties sur stdout sont retournées comme un RDD de chaînes. |
| **coalesce**(_numPartitions_) | Diminue le nombre de partitions dans le RDD à numPartitions. Utile pour exécuter des opérations plus efficacement après avoir filtré un large dataset. |
| **repartition**(_numPartitions_) | Remélange aléatoirement les données dans le RDD pour créer plus ou moins de partitions et les équilibre à travers elles. Ceci remélange toujours toutes les données sur le réseau. |
| **repartitionAndSortWithinPartitions**(_partitioner_) | Repartitionne le RDD selon le partitionneur donné et, à l'intérieur de chaque partition résultante, trie les enregistrements par leurs clés. Ceci est plus efficace que d'appeler `repartition` puis de trier à l'intérieur de chaque partition car il peut pousser le tri dans la machinerie de mélange. |

## Notes importantes

### Transformations lazy (paresseuses)
Toutes les transformations dans Spark sont **lazy**, ce qui signifie qu'elles ne calculent pas leurs résultats immédiatement. Au lieu de cela, elles se "souviennent" simplement des transformations appliquées à un dataset de base (par exemple, un fichier). Les transformations ne sont calculées que lorsqu'une action nécessite qu'un résultat soit retourné au programme pilote.

### Optimisations
Cette conception permet à Spark d'être plus efficace. Par exemple, nous pouvons réaliser qu'un dataset créé via `map` sera utilisé dans un `reduce` et ne retourner que le résultat du `reduce` au pilote, plutôt que le plus grand dataset mappé.

### Persistance
Par défaut, chaque RDD transformé peut être recalculé chaque fois que vous exécutez une action dessus. Cependant, vous pouvez également persister un RDD en mémoire en utilisant les méthodes `persist` (ou `cache`), auquel cas Spark conservera les éléments sur le cluster pour un accès beaucoup plus rapide la prochaine fois que vous l'interrogerez