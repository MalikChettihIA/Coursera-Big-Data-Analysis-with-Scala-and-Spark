# L'opération reduce dans Spark

## Définition

`reduce` est une **action** de réduction qui combine tous les éléments d'un RDD en utilisant une fonction associative et commutative pour produire une seule valeur du même type que les éléments du RDD.

## Signature

```scala
def reduce(f: (T, T) => T): T
```

### Paramètres
- **`f: (T, T) => T`** : Fonction qui prend deux éléments de type T et retourne un élément de type T

## Caractéristiques clés

- **Action** : S'exécute immédiatement (eager)
- **Même type** : Le résultat a le même type que les éléments du RDD
- **Pas de valeur initiale** : Contrairement à `fold`, pas de zeroValue
- **Distribué** : S'exécute en parallèle sur toutes les partitions
- **Contrainte** : La fonction doit être **associative** et **commutative**

## Comment ça fonctionne

### Exécution distribuée
```scala
val numbers = sc.parallelize(List(1, 2, 3, 4, 5, 6), 3)
val sum = numbers.reduce(_ + _)
```

```
RDD: [1, 2, 3, 4, 5, 6] avec 3 partitions

Partition 1: [1, 2]     → reduce(_ + _) → 1 + 2 = 3
Partition 2: [3, 4]     → reduce(_ + _) → 3 + 4 = 7  
Partition 3: [5, 6]     → reduce(_ + _) → 5 + 6 = 11

Combinaison finale: reduce(_ + _) sur [3, 7, 11] → ((3 + 7) + 11) = 21
```

## Exemples pratiques

### Exemple 1 : Opérations arithmétiques
```scala
val numbers = sc.parallelize(List(10, 20, 30, 40, 50))

// Somme
val sum = numbers.reduce(_ + _)
println(s"Somme: $sum")  // Somme: 150

// Produit
val product = numbers.reduce(_ * _)
println(s"Produit: $product")  // Produit: 12000000

// Maximum
val max = numbers.reduce(math.max)
println(s"Maximum: $max")  // Maximum: 50

// Minimum
val min = numbers.reduce(math.min)
println(s"Minimum: $min")  // Minimum: 10
```

### Exemple 2 : Opérations sur chaînes
```scala
val words = sc.parallelize(List("Spark", "Scala", "Hadoop", "Kafka"))

// Concaténation (attention à l'ordre !)
val concatenated = words.reduce(_ + " " + _)
println(s"Concaténé: $concatenated")  // Ordre imprévisible !

// Mot le plus long
val longest = words.reduce((a, b) => if (a.length >= b.length) a else b)
println(s"Plus long: $longest")  // Plus long: Hadoop
```

### Exemple 3 : Objets complexes
```scala
case class Person(name: String, age: Int, salary: Double)

val people = sc.parallelize(List(
  Person("Alice", 25, 50000),
  Person("Bob", 30, 60000),
  Person("Charlie", 35, 70000),
  Person("Diana", 28, 55000)
))

// Personne avec le salaire le plus élevé
val highestPaid = people.reduce((p1, p2) => 
  if (p1.salary >= p2.salary) p1 else p2
)
println(s"Salaire max: $highestPaid")  // Person(Charlie,35,70000.0)

// Personne la plus âgée
val oldest = people.reduce((p1, p2) => 
  if (p1.age >= p2.age) p1 else p2
)
println(s"Plus âgé: $oldest")  // Person(Charlie,35,70000.0)
```

### Exemple 4 : Collections imbriquées
```scala
val listOfLists = sc.parallelize(List(
  List(1, 2, 3),
  List(4, 5),
  List(6, 7, 8, 9)
))

// Concaténer toutes les listes
val flattened = listOfLists.reduce(_ ++ _)
println(s"Aplati: $flattened")  // List(1, 2, 3, 4, 5, 6, 7, 8, 9)
```

### Exemple 5 : Sets et Maps
```scala
// Union de Sets
val sets = sc.parallelize(List(
  Set(1, 2, 3),
  Set(3, 4, 5),
  Set(5, 6, 7)
))

val unionSet = sets.reduce(_ ++ _)
println(s"Union: $unionSet")  // Set(5, 1, 6, 2, 7, 3, 4)

// Fusion de Maps
val maps = sc.parallelize(List(
  Map("a" -> 1, "b" -> 2),
  Map("b" -> 3, "c" -> 4),
  Map("c" -> 5, "d" -> 6)
))

val mergedMap = maps.reduce { (map1, map2) =>
  map2.foldLeft(map1) { case (acc, (key, value)) =>
    acc + (key -> (acc.getOrElse(key, 0) + value))
  }
}
println(s"Maps fusionnées: $mergedMap")  // Map(a -> 1, b -> 5, c -> 9, d -> 6)
```

## Contraintes importantes

### 1. Fonction doit être associative
```scala
// ✅ Addition est associative: (a + b) + c = a + (b + c)
numbers.reduce(_ + _)

// ❌ Soustraction n'est PAS associative: (a - b) - c ≠ a - (b - c)
numbers.reduce(_ - _)  // Résultat imprévisible !
```

### 2. Fonction doit être commutative
```scala
// ✅ Addition est commutative: a + b = b + a
numbers.reduce(_ + _)

// ❌ Division n'est PAS commutative: a / b ≠ b / a
numbers.reduce(_ / _)  // Résultat imprévisible !
```

### 3. RDD ne doit pas être vide
```scala
val emptyRDD = sc.parallelize(List.empty[Int])

// ❌ Exception ! reduce sur RDD vide
val result = emptyRDD.reduce(_ + _)  // UnsupportedOperationException
```

## Gestion des RDD vides

### Option 1 : Vérification préalable
```scala
val numbers = sc.parallelize(List.empty[Int])

if (!numbers.isEmpty()) {
  val sum = numbers.reduce(_ + _)
  println(s"Somme: $sum")
} else {
  println("RDD vide !")
}
```

### Option 2 : Utiliser fold avec valeur par défaut
```scala
val numbers = sc.parallelize(List.empty[Int])

// fold ne lève pas d'exception sur RDD vide
val sum = numbers.fold(0)(_ + _)  // Retourne 0
println(s"Somme: $sum")
```

### Option 3 : Utiliser aggregate
```scala
val numbers = sc.parallelize(List.empty[Int])

val sum = numbers.aggregate(0)(_ + _, _ + _)  // Retourne 0
println(s"Somme: $sum")
```

## Différences avec d'autres opérations

| Opération | Valeur initiale | RDD vide | Type de sortie | Cas d'usage |
|-----------|-----------------|----------|----------------|-------------|
| **`reduce`** | ❌ Non | ❌ Exception | Même type | Agrégations simples |
| **`fold`** | ✅ Oui | ✅ OK | Même type | Comme reduce + sécurité |
| **`aggregate`** | ✅ Oui | ✅ OK | Type différent | Agrégations complexes |

### Comparaison pratique
```scala
val numbers = sc.parallelize(List(1, 2, 3, 4, 5))

// reduce - pas de valeur initiale
val sum1 = numbers.reduce(_ + _)              // 15

// fold - avec valeur initiale 0
val sum2 = numbers.fold(0)(_ + _)             // 15

// aggregate - peut changer de type
val sum3 = numbers.aggregate(0.0)(_ + _, _ + _)  // 15.0 (Double)
```

## Optimisations Spark

### Tree Reduce
Pour de très gros RDD, Spark peut utiliser un "tree reduce" pour éviter qu'un seul executor doive traiter tous les résultats intermédiaires :

```scala
// Réduction en arbre pour de meilleures performances
val hugResult = hugeRDD.treeReduce(_ + _)
```

### Partitioning
```scala
// Contrôler le nombre de partitions pour optimiser reduce
val numbers = sc.parallelize(List(1 to 1000000), numSlices = 100)
val sum = numbers.reduce(_ + _)
```

## Cas d'usage typiques

### ✅ Bons cas d'usage
- **Sommes, produits** : `reduce(_ + _)`, `reduce(_ * _)`
- **Min/Max** : `reduce(math.min)`, `reduce(math.max)`
- **Union de collections** : `reduce(_ ++ _)` pour Sets/Lists
- **Recherche d'éléments** : Plus grand élément selon un critère

### ❌ Mauvais cas d'usage
- **Opérations non-associatives** : soustraction, division
- **Concaténation de chaînes** : ordre imprévisible
- **RDD potentiellement vides** : préférer `fold` ou `aggregate`
- **Changement de type** : utiliser `aggregate`

## Exemples avancés

### Calcul de moyennes avec reduce
```scala
case class Stats(sum: Double, count: Int) {
  def +(other: Stats) = Stats(sum + other.sum, count + other.count)
  def average = if (count > 0) sum / count else 0.0
}

val numbers = sc.parallelize(List(10, 20, 30, 40, 50))

val stats = numbers
  .map(x => Stats(x, 1))  // Transformer chaque nombre en Stats
  .reduce(_ + _)          // Réduire avec l'opérateur + défini

println(s"Moyenne: ${stats.average}")  // Moyenne: 30.0
```

### Recherche d'éléments complexes
```scala
case class Student(name: String, grade: Double, subjects: List[String])

val students = sc.parallelize(List(
  Student("Alice", 85.5, List("Math", "Science")),
  Student("Bob", 92.0, List("History", "Art")),
  Student("Charlie", 78.5, List("Math", "Music"))
))

// Étudiant avec la meilleure note
val topStudent = students.reduce((s1, s2) => 
  if (s1.grade >= s2.grade) s1 else s2
)
println(s"Meilleur étudiant: $topStudent")
```

## Bonnes pratiques

### ✅ DO
```scala
// Utiliser des opérations associatives et commutatives
numbers.reduce(_ + _)
numbers.reduce(math.max)
sets.reduce(_ union _)

// Vérifier si RDD non vide pour éviter les exceptions
if (!rdd.isEmpty()) {
  val result = rdd.reduce(operation)
}
```

### ❌ DON'T
```scala
// Éviter les opérations non-associatives
numbers.reduce(_ - _)  // Imprévisible !

// Éviter sur RDD potentiellement vides
emptyRDD.reduce(_ + _)  // Exception !

// Éviter pour changer de type
numbers.reduce((acc, x) => acc.toString + x)  // Erreur de compilation !
```

## Résumé

`reduce` est l'opération de réduction la plus simple et directe dans Spark. Elle est parfaite pour des agrégations basiques (somme, max, min) sur des RDD dont on sait qu'ils ne sont pas vides. Pour des cas plus complexes ou plus sûrs, préférer `fold` ou `aggregate`.