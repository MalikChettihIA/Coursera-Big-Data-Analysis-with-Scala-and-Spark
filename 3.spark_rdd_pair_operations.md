# Pair RDD Operations dans Spark

## Définition

Les **Pair RDD Operations** sont des opérations spécialisées qui travaillent sur des RDD contenant des **paires clé-valeur** (tuples de 2 éléments). Ces opérations permettent de manipuler et d'agréger des données basées sur leurs clés.

## Structure d'un Pair RDD

```scala
// Type d'un Pair RDD
RDD[(K, V)]  // où K = type de la clé, V = type de la valeur

// Exemples
RDD[(String, Int)]     // Clé String, Valeur Int
RDD[(Int, Person)]     // Clé Int, Valeur Person  
RDD[(String, List[String])]  // Clé String, Valeur List[String]
```

## Création de Pair RDD

### Méthode 1 : Directement à partir de données
```scala
val pairs = sc.parallelize(List(
  ("apple", 5),
  ("banana", 3),
  ("apple", 8),
  ("orange", 2),
  ("banana", 1)
))
// Type: RDD[(String, Int)]
```

### Méthode 2 : Transformation d'un RDD existant
```scala
val words = sc.parallelize(List("hello", "world", "hello", "spark"))

// Créer des paires (mot, 1) pour comptage
val wordPairs = words.map(word => (word, 1))
// Type: RDD[(String, Int)]

// Créer des paires (première lettre, mot)
val letterPairs = words.map(word => (word.charAt(0), word))
// Type: RDD[(Char, String)]
```

### Méthode 3 : À partir de fichiers
```scala
// Lire un fichier et créer des paires (ligne_numéro, contenu)
val lines = sc.textFile("hdfs://...")
val numberedLines = lines.zipWithIndex().map(_.swap)
// Type: RDD[(Long, String)]
```

## Opérations spécialisées sur Pair RDD

### 1. Transformations par clé

#### `groupByKey()` - Grouper par clé
```scala
val sales = sc.parallelize(List(
  ("apple", 5), ("banana", 3), ("apple", 8), ("banana", 1)
))

val grouped = sales.groupByKey()
// Résultat: RDD[(String, Iterable[Int])]
// ("apple", [5, 8])
// ("banana", [3, 1])

grouped.collect().foreach { case (fruit, values) =>
  println(s"$fruit: ${values.mkString(", ")}")
}
```

#### `reduceByKey()` - Réduire par clé
```scala
val sales = sc.parallelize(List(
  ("apple", 5), ("banana", 3), ("apple", 8), ("banana", 1)
))

val totals = sales.reduceByKey(_ + _)
// Résultat: RDD[(String, Int)]
// ("apple", 13)
// ("banana", 4)

totals.collect().foreach { case (fruit, total) =>
  println(s"$fruit: $total")
}
```

#### `aggregateByKey()` - Agrégation avancée par clé
```scala
val scores = sc.parallelize(List(
  ("Alice", 85), ("Bob", 92), ("Alice", 90), ("Bob", 88), ("Alice", 95)
))

// Calculer (somme, count) pour chaque étudiant
val stats = scores.aggregateByKey((0, 0))(
  seqOp = (acc, score) => (acc._1 + score, acc._2 + 1),
  combOp = (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
)

val averages = stats.mapValues { case (sum, count) => sum.toDouble / count }
averages.collect().foreach { case (name, avg) =>
  println(s"$name: moyenne = $avg")
}
```

#### `combineByKey()` - Combinaison flexible par clé
```scala
val scores = sc.parallelize(List(
  ("Math", 85), ("Science", 92), ("Math", 90), ("Science", 88)
))

val averages = scores.combineByKey(
  createCombiner = (score: Int) => (score, 1),
  mergeValue = (acc: (Int, Int), score: Int) => (acc._1 + score, acc._2 + 1),
  mergeCombiners = (acc1: (Int, Int), acc2: (Int, Int)) => 
    (acc1._1 + acc2._1, acc1._2 + acc2._2)
).mapValues { case (sum, count) => sum.toDouble / count }
```

### 2. Opérations de tri

#### `sortByKey()` - Trier par clé
```scala
val pairs = sc.parallelize(List(("c", 3), ("a", 1), ("b", 2)))

val sorted = pairs.sortByKey()
// Résultat: ("a", 1), ("b", 2), ("c", 3)

val sortedDesc = pairs.sortByKey(ascending = false)
// Résultat: ("c", 3), ("b", 2), ("a", 1)
```

### 3. Opérations sur les valeurs

#### `mapValues()` - Transformer les valeurs
```scala
val prices = sc.parallelize(List(
  ("apple", 1.20), ("banana", 0.80), ("orange", 1.50)
))

// Appliquer une taxe de 20%
val pricesWithTax = prices.mapValues(_ * 1.20)
pricesWithTax.collect().foreach { case (fruit, price) =>
  println(s"$fruit: €$price")
}
```

#### `flatMapValues()` - Transformer et aplatir les valeurs
```scala
val sentences = sc.parallelize(List(
  ("doc1", "hello world spark"),
  ("doc2", "scala programming language")
))

val words = sentences.flatMapValues(_.split(" "))
// Résultat:
// ("doc1", "hello"), ("doc1", "world"), ("doc1", "spark")
// ("doc2", "scala"), ("doc2", "programming"), ("doc2", "language")
```

### 4. Jointures

#### `join()` - Jointure interne
```scala
val names = sc.parallelize(List((1, "Alice"), (2, "Bob"), (3, "Charlie")))
val ages = sc.parallelize(List((1, 25), (2, 30), (4, 35)))

val joined = names.join(ages)
// Résultat: RDD[(Int, (String, Int))]
// (1, ("Alice", 25)), (2, ("Bob", 30))
// Note: (3, "Charlie") et (4, 35) sont exclus (pas de clé correspondante)
```

#### `leftOuterJoin()` - Jointure externe gauche
```scala
val leftJoined = names.leftOuterJoin(ages)
// Résultat: RDD[(Int, (String, Option[Int]))]
// (1, ("Alice", Some(25))), (2, ("Bob", Some(30))), (3, ("Charlie", None))
```

#### `rightOuterJoin()` - Jointure externe droite
```scala
val rightJoined = names.rightOuterJoin(ages)
// Résultat: RDD[(Int, (Option[String], Int))]
// (1, (Some("Alice"), 25)), (2, (Some("Bob"), 30)), (4, (None, 35))
```

#### `fullOuterJoin()` - Jointure externe complète
```scala
val fullJoined = names.fullOuterJoin(ages)
// Résultat: RDD[(Int, (Option[String], Option[Int]))]
// (1, (Some("Alice"), Some(25)))
// (2, (Some("Bob"), Some(30)))
// (3, (Some("Charlie"), None))
// (4, (None, Some(35)))
```

### 5. Opérations d'ensemble

#### `subtractByKey()` - Soustraction par clé
```scala
val rdd1 = sc.parallelize(List(("a", 1), ("b", 2), ("c", 3)))
val rdd2 = sc.parallelize(List(("a", 4), ("c", 6)))

val result = rdd1.subtractByKey(rdd2)
// Résultat: ("b", 2) - seules les clés absentes de rdd2
```

#### `cogroup()` - Regroupement par clé
```scala
val rdd1 = sc.parallelize(List(("a", 1), ("a", 2), ("b", 3)))
val rdd2 = sc.parallelize(List(("a", 4), ("c", 5)))

val cogrouped = rdd1.cogroup(rdd2)
// Résultat: RDD[(String, (Iterable[Int], Iterable[Int]))]
// ("a", ([1, 2], [4]))
// ("b", ([3], []))
// ("c", ([], [5]))
```

## Actions spécialisées

### `collectAsMap()` - Collecter comme Map
```scala
val pairs = sc.parallelize(List(("a", 1), ("b", 2), ("c", 3)))
val map = pairs.collectAsMap()
// Résultat: Map("a" -> 1, "b" -> 2, "c" -> 3)
```

### `countByKey()` - Compter par clé
```scala
val data = sc.parallelize(List(("a", 1), ("b", 2), ("a", 3), ("c", 4)))
val counts = data.countByKey()
// Résultat: Map("a" -> 2, "b" -> 1, "c" -> 1)
```

### `lookup()` - Rechercher par clé
```scala
val pairs = sc.parallelize(List(("a", 1), ("a", 2), ("b", 3)))
val values = pairs.lookup("a")
// Résultat: Seq(1, 2)
```

## Exemple complet : Analyse de logs

```scala
// Simuler des logs d'accès web
case class LogEntry(ip: String, url: String, status: Int, size: Long)

val logs = sc.parallelize(List(
  LogEntry("192.168.1.1", "/home", 200, 1024),
  LogEntry("192.168.1.2", "/about", 200, 512),
  LogEntry("192.168.1.1", "/contact", 404, 256),
  LogEntry("192.168.1.3", "/home", 200, 2048),
  LogEntry("192.168.1.2", "/login", 500, 128)
))

// 1. Créer des pairs (IP, taille)
val ipSizes = logs.map(log => (log.ip, log.size))

// 2. Calculer le total de données par IP
val totalByIP = ipSizes.reduceByKey(_ + _)

// 3. Créer des pairs (status, 1) pour compter
val statusCounts = logs.map(log => (log.status, 1)).reduceByKey(_ + _)

// 4. Joindre avec des informations d'utilisateur
val userInfo = sc.parallelize(List(
  ("192.168.1.1", "Alice"),
  ("192.168.1.2", "Bob"),
  ("192.168.1.3", "Charlie")
))

val userTraffic = totalByIP.join(userInfo)
// Résultat: (IP, (total_bytes, nom_utilisateur))

println("Trafic par utilisateur:")
userTraffic.collect().foreach { case (ip, (bytes, name)) =>
  println(s"$name ($ip): ${bytes} bytes")
}

println("\nStatuts HTTP:")
statusCounts.collect().foreach { case (status, count) =>
  println(s"Status $status: $count requêtes")
}
```

## Avantages des Pair RDD Operations

### ✅ Avantages
- **Optimisations** : Spark optimise automatiquement les opérations par clé
- **Partitioning** : Données peuvent être partitionnées par clé pour de meilleures performances
- **Jointures efficaces** : Opérations de jointure optimisées
- **Agrégations** : Parfait pour des calculs type MapReduce

### ⚠️ Considérations
- **Mémoire** : `groupByKey()` peut consommer beaucoup de mémoire
- **Performance** : `reduceByKey()` > `groupByKey()` pour les agrégations
- **Partitioning** : Utiliser `partitionBy()` pour optimiser les opérations répétées

## Bonnes pratiques

### ✅ DO
```scala
// Préférer reduceByKey à groupByKey pour les agrégations
data.reduceByKey(_ + _)  // ✅ Efficient

// Utiliser mapValues pour transformer seulement les valeurs
pairs.mapValues(_ * 2)   // ✅ Préserve le partitioning

// Persister les Pair RDD réutilisés
val pairs = data.map(x => (x.key, x.value)).persist()
```

### ❌ DON'T
```scala
// Éviter groupByKey si vous voulez juste agréger
data.groupByKey().mapValues(_.sum)  // ❌ Inefficient

// Éviter de recréer les pairs inutilement
data.map(x => (x.key, x.value)).map(x => (x._1, x._2 * 2))  // ❌ Redondant
```

## Résumé

Les **Pair RDD Operations** sont essentielles pour traiter des données structurées clé-valeur dans Spark. Elles permettent des opérations puissantes comme les jointures, agrégations par clé, et tri, tout en bénéficiant des optimisations de partitioning de Spark. C'est la base de beaucoup d'applications Big Data et d'algorithmes MapReduce.